Dimensionality Reduction Techniques

1. What is dimensionality reduction? Why is it important in machine learning?
    Dimensionality reduction is the process of reducing the number of features or variables in a dataset while retaining its essential characteristics. 
    It is important in machine learning because it helps to reduce overfitting, improve model performance, and decrease computational costs. 
    By eliminating irrelevant or redundant features, dimensionality reduction can lead to simpler models that generalize better to unseen data.

2. Explain the term "curse of dimensionality" with an example.
        The curse of dimensionality refers to the problem that arises when working with data in high-dimensional spaces.
    As the number of features (dimensions) increases:

        - The dataset becomes sparser
        - Computation becomes more complex
        - The risk of overfitting increases
        - It becomes harder to visualize and analyze the data

    Eg - data with 2 features vs data with 100 features.
        To reduce the curse of dimensionality techniques like PCA , LDA or feature selection are used.

3. List any four benefits of dimensionality reduction.
   - Reduces overfitting by simplifying the model
   - Improves model performance by removing irrelevant features or redundant features
   - Decreases computational cost by reducing the number of features
   - Enhances data visualization by projecting high-dimensional data into lower dimensions

4. Mention any three disadvantages of dimensionality reduction.
        1. ‚ùå Loss of Information
             Important data may be discarded, which can reduce model accuracy.

        2. ‚ùå Reduced Interpretability
         New features (like principal components) are often abstract and hard to understand.

        3. ‚ùå Irreversibility
         Once reduced, it's difficult to reconstruct the original dataset accurately.

5. What is the difference between feature selection and feature extraction?
    - **Feature Selection**: Involves selecting a subset of the original features based on certain criteria
            (e.g., correlation, importance). The original features remain unchanged.
    - **Feature Extraction**: Involves transforming the original features into a new set of features
             (e.g., PCA) that captures the essential information in fewer dimensions. The original features are not retained.

---


### üîπ **Explain/Describe Questions**

6. Describe the three main approaches to feature selection.
#### ‚úÖ **1. Filter Methods**

 **How it works**: Uses statistical techniques to score and rank features based on relevance.
 **Independent of ML model**.
 **Common techniques**:

  * Correlation Coefficient
  * Chi-Square Test
  * ANOVA
  * Information Gain
 **Pros**: Fast, simple, avoids overfitting.

---

#### ‚úÖ **2. Wrapper Methods**

* **How it works**: Uses an ML model to evaluate feature subsets by training the model multiple times.
* **Model-dependent and computationally expensive**.
* **Common techniques**:

  * Forward Selection
  * Backward Elimination
  * Bi-directional Elimination
* **Pros**: More accurate than filter methods.


#### ‚úÖ **3. Embedded Methods**

* **How it works**: Integrates feature selection as part of the model training process.
* **Uses model coefficients or importance scores**.
* **Common techniques**:

  * LASSO (L1 Regularization)
  * Ridge Regression
  * Elastic Net
* **Pros**: Balances speed and accuracy; considers feature interaction.

---

These methods help reduce overfitting, improve accuracy, and enhance model interpretability.

7. Explain the working of filter methods with suitable techniques.

 üîπ **ANOVA (Analysis of Variance)**
Used to determine whether the **means of a numerical feature differ significantly across categories** using the **p-value**.
 üîπ **Chi-Square Test**
Checks the **association between two categorical variables** by comparing observed and expected frequencies.
 üîπ **Correlation Coefficient**
Measures the **strength and direction of the linear relationship** between two numerical variables (e.g., Pearson's r).
 üîπ **Information Gain**
Calculates the **reduction in entropy (uncertainty)** after splitting a dataset based on a feature; used in decision trees.

8. Describe wrapper methods in detail with an example.
    Wrapper methods evaluate feature subsets by training a model and assessing its performance. 
    They use a specific machine learning algorithm to evaluate the quality of the selected features. 
    The process involves:

    1. **Subset Selection**: Start with an empty set or all features.
    2. **Model Training**: Train the model using the selected subset.
    3. **Performance Evaluation**: Evaluate the model's performance using metrics like accuracy, F1-score, etc.
    4. **Iteration**: Add or remove features based on performance until the optimal subset is found.

    Example: Forward Selection
        - Start with no features.
        - Add one feature at a time that improves model performance the most.
        - Stop when adding more features does not improve performance.    
9. What are embedded methods? Mention two techniques used under it.
    Embedded methods integrate feature selection into the model training process. 
    They automatically select features based on their importance during model training. 
    This approach is efficient and considers feature interactions.

    Two techniques used in embedded methods are:
    - **LASSO (L1 Regularization)**: Adds a penalty term to the loss function, forcing some feature coefficients to be zero, effectively selecting features.
    - **Ridge Regression**: Similar to LASSO but uses L2 regularization, which shrinks coefficients but does not set them to zero.
    It helps in multicollinearity.
10. Explain Principal Component Analysis (PCA) and its need.

Principal Component Analysis (PCA) is a **dimensionality reduction technique** that transforms high-dimensional data into a 
**lower-dimensional form** by creating new features (called **principal components**)
 which retain most of the original data‚Äôs variance.

    - It uses **orthogonal transformation** to convert correlated variables into a set of **uncorrelated** ones.
    - These new features are **linear combinations** of the original features.



#### ‚úÖ **Need for PCA:**

1. **Handles Curse of Dimensionality**: Reduces the number of features to prevent overfitting and improve generalization.
2. **Improves Computation**: Speeds up model training by reducing feature count.
3. **Enhances Visualization**: Projects high-dimensional data into 2D or 3D for better understanding.
4. **Removes Multicollinearity**: Converts correlated features into uncorrelated principal components.

---

üß† In short:
PCA helps simplify complex datasets while **retaining maximum useful information**.


11. Compare Forward Selection and Backward Elimination.
| Aspect                 | **Forward Selection**                                 | **Backward Elimination**                               |
| ---------------------- | ----------------------------------------------------- | ------------------------------------------------------ |
| **Starting Point**     | Starts with **no features**                           | Starts with **all features**                           |
| **Approach**           | **Adds** one feature at a time                        | **Removes** one feature at a time                      |
| **Selection Criteria** | Adds feature that **improves model performance most** | Removes feature that **hurts model performance least** |
| **When to Stop**       | When no improvement in performance                    | When removal worsens performance                       |
| **Efficiency**         | Faster when few relevant features exist               | Better when many irrelevant features are present       |


12. Differentiate between PCA and LDA.

| Aspect                  | **PCA (Principal Component Analysis)**                     | **LDA (Linear Discriminant Analysis)**                         |
| ----------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- |
| **Type**                | Unsupervised technique                                     | Supervised technique                                           |
| **Objective**           | Maximizes **variance** in the data                         | Maximizes **class separability**                               |
| **Input Consideration** | Does **not use class labels**                              | Uses **class labels**                                          |
| **Feature Limit**       | Can produce up to **n** components (n = original features) | Can produce up to **C ‚Äì 1** components (C = number of classes) |
| **Use Case**            | Data compression, visualization                            | Classification problems                                        |

‚úÖ In short:

* **PCA** focuses on preserving information.
* **LDA** focuses on improving **classification**.

13. How is Kernel PCA different from standard PCA?

| Aspect                | **PCA (Principal Component Analysis)**      | **Kernel PCA**                               |
| --------------------- | ------------------------------------------- | -------------------------------------------- |
| **Type**              | Linear dimensionality reduction             | Non-linear dimensionality reduction          |
| **Data Handling**     | Works well with **linearly separable** data | Works with **non-linear** data structures    |
| **Transformation**    | Uses **linear projections**                 | Uses **kernel functions** to project data    |
| **Kernel Used**       | ‚ùå No kernel used                            | ‚úÖ Uses kernels like RBF, polynomial, sigmoid |
| **Complexity**        | Computationally efficient                   | More complex and computationally intensive   |
| **Inverse Transform** | Easy to reverse                             | Difficult or impossible to reverse           |

‚úÖ In short:

* **PCA** works in the original feature space.
* **Kernel PCA** projects data into a **higher-dimensional space** to capture non-linear patterns.


14. Compare feature selection and feature extraction with one advantage and one disadvantage each.
| Aspect                 | Feature Selection                                  | Feature Extraction                                 |
| ---------------------- | -------------------------------------------------- | -------------------------------------------------- |
| **Definition**         | Selects a subset of **existing original features** | Creates **new features** from the original set     |
| **Approach**           | Keeps only the **most relevant** features          | Transforms data into a **lower-dimensional space** |
| **Data Structure**     | Original feature structure is **unchanged**        | Original features are **transformed or combined**  |
| **Example Techniques** | Chi-Square Test, LASSO, Forward Selection          | PCA, LDA, Kernel PCA                               |
| **Interpretability**   | Easier to interpret (original features kept)       | Harder to interpret (features may be abstract)     |

| **Advantage**          | ‚úÖ Easy to interpret (original features are retained) | ‚úÖ Captures more information in fewer features |
| **Disadvantage**       | ‚ùå May miss hidden patterns in combined features      | ‚ùå New features are often hard to interpret    |

‚úÖ In short:

* Use **feature selection** when interpretability is important.
* Use **feature extraction** when accuracy or compression is the priority.

Explain LDA 
Explain LRA (Low rank approximation)


### üîπ **Application/Use-Case Based**

15. Why would you use dimensionality reduction in a dataset with thousands of features?
16. A dataset has multicollinearity issues ‚Äî which dimensionality reduction technique would help and why?
17. Which technique is more suitable for visualizing high-dimensional data: PCA or t-SNE? Justify your answer.

---

### üîπ **True/False / MCQ Style**

18. Dimensionality reduction increases overfitting. (True/False)
FALSE
19. LASSO is an example of which type of feature selection method?

    * a) Filter
    * b) Wrapper
    * c) Embedded 
    * d) Hybrid

20. Which of the following is not a feature extraction technique?

    * a) PCA
    * b) Chi-Square Test
    * c) LDA
    * d) Autoencoder

---
